{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HExLQrE4ZxR"
   },
   "source": [
    "<h1><font color='blue'> 8E and 8F: Finding the Probability P(Y==1|X)</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LuKrFzC4ZxV"
   },
   "source": [
    "<h2><font color='Geen'> 8E: Implementing Decision Function of SVM RBF Kernel</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wES-wWN4ZxX"
   },
   "source": [
    "<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n",
    "\n",
    "Check the documentation for better understanding of these attributes: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "<img src='https://i.imgur.com/K11msU4.png' width=500>\n",
    "\n",
    "As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n",
    "\n",
    "Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n",
    "\n",
    "Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n",
    "\n",
    "Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n",
    "$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n",
    "\n",
    "RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n",
    "\n",
    "For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z830CfMk4Zxa"
   },
   "source": [
    "## Task E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuBxHiCQ4Zxc"
   },
   "source": [
    "> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n",
    "\n",
    "> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n",
    "\n",
    "> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fCgMNEvI4Zxf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ANUNIqCe4Zxn"
   },
   "outputs": [],
   "source": [
    "X, Y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state = 0)\n",
    "X_train, X_cv, Y_train, Y_cv = train_test_split(X_train, Y_train, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.29548643  0.42093955  0.04696382  0.0792216   0.20891604]\n",
      " [-0.6466714  -0.10720801  0.11643484  0.14071315 -0.44805596]\n",
      " [-0.87471418 -0.61370156  0.16909783  0.18219095 -1.03516269]\n",
      " ...\n",
      " [ 0.23263333  0.24367062  0.43482552  0.55666254 -1.13265111]\n",
      " [ 0.12084262 -0.08221712 -0.11956927 -0.1538085   0.29868972]\n",
      " [ 0.14241304 -1.37526811 -0.50505122 -0.69942701  0.39873757]]\n"
     ]
    }
   ],
   "source": [
    "print(X_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHie1zqH4Zxt"
   },
   "source": [
    "### Pseudo code\n",
    "\n",
    "clf = SVC(gamma=0.001, C=100.)<br>\n",
    "clf.fit(Xtrain, ytrain)\n",
    "\n",
    "<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n",
    "   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n",
    "    \n",
    "fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n",
    "\n",
    "<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h43kDT3M41u5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100.0, gamma=0.001)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can write your code here\n",
    "clf = SVC(gamma=0.001, C=100., kernel='rbf')\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2181721212162735\n"
     ]
    }
   ],
   "source": [
    "intercept_train = clf.intercept_[0]\n",
    "print(intercept_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_train = clf.dual_coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.38799984 -1.62143033 -2.74101189 ... -3.41768652  0.23102012\n",
      "  1.2823461 ]\n",
      "1050\n"
     ]
    }
   ],
   "source": [
    "y_pred_clf = clf.decision_function(X_cv)\n",
    "print(y_pred_clf)\n",
    "print(len(y_pred_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_indices = clf.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_temp = Y_train[support_indices]\n",
    "X_temp = X_train[support_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(469, 5)\n",
      "(469,)\n",
      "(1050, 5)\n",
      "(469,)\n"
     ]
    }
   ],
   "source": [
    "print(X_temp.shape)\n",
    "print(Y_temp.shape)\n",
    "print(X_cv.shape)\n",
    "print(alpha_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_function(X_cv):\n",
    "    \n",
    "    x_cv_pred=np.empty([1000,0])\n",
    "    for i in X_cv:\n",
    "        sum=0\n",
    "        for j in range(0,len(X_temp)):\n",
    "            k = np.exp(-0.001 * (np.dot(X_temp[j]-i, X_temp[j]-i)))\n",
    "            sum += (alpha_train[j] * k)\n",
    "            \n",
    "        x_cv_pred=np.append(x_cv_pred,(sum+intercept_train))\n",
    "    return x_cv_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K(xi, xcv):\n",
    "    gamma = 0.0001\n",
    "    sum = 0\n",
    "    for i in range(len(xi)):\n",
    "        diff = abs(xi[i] - xcv[i])\n",
    "        sum += math.exp(-gamma * diff**2)\n",
    "        \n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.38799984 -1.62143033 -2.74101189 ... -3.41768652  0.23102012\n",
      "  1.2823461 ]\n",
      "(1050,)\n"
     ]
    }
   ],
   "source": [
    "fcv = decision_function(X_cv)\n",
    "print(fcv)\n",
    "print(fcv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_clf - fcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.46148954  0.13255177 -1.64833109 ... -3.02145553 -3.12961766\n",
      " -1.53950159]\n",
      "(1500,)\n"
     ]
    }
   ],
   "source": [
    "ftest = decision_function(X_test)\n",
    "print(ftest)\n",
    "print(ftest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0bKCboN4Zxu"
   },
   "source": [
    "<h2><font color='Geen'> 8F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMn7OEN94Zxw"
   },
   "source": [
    "Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n",
    "<img src='https://i.imgur.com/CAMnVnh.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0n5EFkx4Zxz"
   },
   "source": [
    "## TASK F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0HOqVJq4Zx1"
   },
   "source": [
    "\n",
    "> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n",
    "\n",
    "> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n",
    "\n",
    "> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n",
    "<img src='https://i.imgur.com/zKYE9Oc.png'>\n",
    "if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n",
    "\n",
    "> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2450,)\n",
      "(1050,)\n",
      "[0.00134953 0.00134953 0.00134953 ... 0.00134953 0.00134953 0.99680511]\n",
      "(1500,)\n",
      "[0.00098328 0.00098328 0.00098328 ... 0.00098328 0.00098328 0.00098328]\n"
     ]
    }
   ],
   "source": [
    "pos = np.count_nonzero(Y_train == 1)\n",
    "neg = np.count_nonzero(Y_train == 0)\n",
    "yp = (pos+1) / (pos+2)\n",
    "yn = 1 / (neg+2)\n",
    "y_updated = np.where(Y_train==0,yn,yp)\n",
    "\n",
    "print(y_updated.shape)\n",
    "\n",
    "pos_cv = np.count_nonzero(Y_cv == 1)\n",
    "neg_cv = np.count_nonzero(Y_cv == 0)\n",
    "yp_cv = (pos_cv+1) / (pos_cv+2)\n",
    "yn_cv = 1 / (neg_cv+2)\n",
    "y_cv_updated = np.where(Y_cv==0,yn_cv,yp_cv)\n",
    "y_cv_updated = y_cv_updated.astype('float')\n",
    "\n",
    "print(y_cv_updated.shape)\n",
    "print(y_cv_updated)\n",
    "\n",
    "pos_test = np.count_nonzero(Y_test == 1)\n",
    "neg_test = np.count_nonzero(Y_test == 0)\n",
    "yp_test = (pos_test+1) / (pos_test+2)\n",
    "yn_test = 1 / (neg_test+2)\n",
    "y_test_updated = np.where(Y_test==0,yn_test,yp_test)\n",
    "\n",
    "print(y_test_updated.shape)\n",
    "print(y_test_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(dim):\n",
    "    w = np.zeros_like(dim)\n",
    "    b = 0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / ( 1 + np.exp(-z) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " def gradient_db(x,y,w,b):\n",
    "    db = y - sigmoid(np.dot(w, x+b ) )\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    dw = x*( y - sigmoid(np.dot(w, x+b ) ) ) - ( ( alpha * w )/ N )\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(y_true,y_pred):\n",
    "    '''In this function, we will compute log loss '''\n",
    "\n",
    "    loss = 0\n",
    "    n = len(y_true)\n",
    "    n=2\n",
    "    for i in range(0,n):\n",
    "#         print(y_true[i], y_pred[i])\n",
    "        loss += ( y_true[i] * math.log10(y_pred[i]) ) + (( 1 - y_true[i] )* math.log10( 1 - y_pred[i] ))\n",
    "        \n",
    "    loss = -1 * (1/n) * loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X,Y,epochs,alpha,eta0):\n",
    "    w,b = initialize_weights(X[0])\n",
    "    y_pred = []\n",
    "    loss_train = []\n",
    "    loss_test = []\n",
    "    N = len(X)\n",
    "    for i in range(epochs):\n",
    "        for j in range(N):\n",
    "            ## batch size of 1\n",
    "            x = X[j]\n",
    "            y = Y[j]\n",
    "            dw = gradient_dw(x,y,w,b,alpha,N)\n",
    "            db = gradient_db(x,y,w,b)\n",
    "            \n",
    "            w += (eta0 * dw)\n",
    "            b += (eta0 * db)\n",
    "            y_pred_cv = sigmoid(np.dot(w.T, x.T) + b )\n",
    "            y_pred.append(y_pred_cv)\n",
    "        \n",
    "        loss_train.append(logloss(Y,y_pred))\n",
    "#         y_pred_test = sigmoid(np.dot(w.T, X_test.T) + b)\n",
    "#         loss_test.append(logloss(y_test,y_test_updated))\n",
    "    return w,b, loss_train, loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: 1.2234896642037565\n",
      "intercept: -0.13413200738785755\n"
     ]
    }
   ],
   "source": [
    "alpha=0.0001\n",
    "eta0=0.0001\n",
    "N=len(fcv)\n",
    "epochs=50\n",
    "w,b, loss_train, loss_test=train(fcv,y_cv_updated,epochs,alpha,eta0)\n",
    "print(\"weight:\", w)\n",
    "print(\"intercept:\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: 1.4000999364229088\n",
      "intercept: -0.027487213803510525\n"
     ]
    }
   ],
   "source": [
    "alpha=0.001\n",
    "eta0=0.0001\n",
    "N=len(ftest)\n",
    "epochs=50\n",
    "w,b, loss_train, loss_test=train(ftest,y_test_updated,epochs,alpha,eta0)\n",
    "print(\"weight:\", w)\n",
    "print(\"intercept:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTY7z2bd4Zx2"
   },
   "source": [
    "__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CM3odN1Z4Zx3"
   },
   "source": [
    "\n",
    "If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n",
    "\n",
    "1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n",
    "\n",
    "2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n",
    "\n",
    "3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n",
    "\n",
    "4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "8E&F_LR_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
